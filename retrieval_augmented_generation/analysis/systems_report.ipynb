{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Retrieval-Augmented Generation System\n",
    "\n",
    "## **1. System Design**:\n",
    "\n",
    "**System Overview**\n",
    "\n",
    "The Retrieval-Augmented Generation (RAG) system is designed to meet the requirements of a scalable, efficient, and accurate question-answering system. It combines document retrieval with a BERT-based generation model, allowing it to handle large-scale corpora while providing accurate, contextually relevant answers.  \n",
    "\n",
    "### **Requirements and How the System Design Meets Them**\n",
    "\n",
    "1. **High Relevancy on Domain-Specific Information**:\n",
    "\n",
    "    - **Design Decision**: The system uses a combination of pre-trained BERT models and KDTree-based retrieval, ensuring that retrieved contexts are highly relevant to the query. The embedding model is chosen to capture domain-specific nuances, and the retrieval system is optimized for speed and accuracy.\n",
    "    - **Outcome**: This approach ensures that the system returns contextually relevant information even in domain-specific scenarios.\n",
    "\n",
    "2. **Adaptation to Dynamic/Growing Information**:\n",
    "\n",
    "    - **Design Decision**: The system allows for dynamic updates to the corpus. New documents can be added, and embeddings can be recalculated and indexed in the KDTree, ensuring the system adapts to changes in the corpus.\n",
    "    - **Outcome**: The system remains up-to-date and accurate as new information is added.\n",
    "\n",
    "3. **Scalability to Billions of Documents**:\n",
    "\n",
    "    - **Design Decision**: The KDTree is used for efficient retrieval, allowing the system to scale to large datasets. The embedding model is lightweight, ensuring fast processing of large volumes of data.\n",
    "    - **Outcome**: The system can handle large-scale document retrieval efficiently.\n",
    "\n",
    "4. **Source Citation**:\n",
    "\n",
    "    - **Design Decision**: Each retrieved chunk of context is stored with metadata, including document and chunk identifiers, allowing the system to cite the source of information accurately.\n",
    "    - **Outcome**: The system can provide users with citations for the information used to generate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **System Components and Processes**\n",
    "\n",
    "1.  **Flask API Interface:**\n",
    "    \n",
    "    -   Provides endpoints for precomputing embeddings, searching for context, generating answers, and evaluating questions.\n",
    "2.  **Pipeline:**\n",
    "    \n",
    "    -   **Preprocessing and Embedding:** Documents are preprocessed (chunking, trimming) and converted into embeddings using Sentence Transformers.\n",
    "    -   **KDTree Retrieval:** Efficient nearest neighbor search retrieves relevant context chunks based on the query embedding.\n",
    "    -   **BERT-based Answer Generation:** Retrieved contexts are fed into a BERT model to generate answers.\n",
    "3.  **Data Storage:**\n",
    "    \n",
    "    -   **Corpus Storage:** Raw documents are stored and processed for embedding.\n",
    "    -   **Embeddings Storage:** Precomputed embeddings are stored for fast retrieval.\n",
    "    -   **Logs:** All API interactions are logged for monitoring and auditing.\n",
    "\n",
    "#### **Complete System Diagram**:\n",
    "The following diagram documents the system. In case you cannot open it, please refer to the analysis/ directory in this project for a valid picture.\n",
    "\n",
    "![System Architecture Diagram](https://github.com/creating-ai-enabled-systems-summer-2024/marquezjaramillo-jose/blob/main/visual_search_system/analysis/system_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data, Data Pipelines, and Model\n",
    "\n",
    "### Data Description\n",
    "\n",
    "**Data Sources:**\n",
    "\n",
    "-   **Document Corpus:** A collection of text documents used as the knowledge base for answering queries.\n",
    "-   **Question-Answer Pairs:** A set of questions and corresponding ground-truth answers used for model evaluation.\n",
    "\n",
    "**Data Characteristics:**\n",
    "\n",
    "-   **Corpus:** Unstructured text data that is preprocessed into smaller chunks.\n",
    "-   **Question-Answer Pairs:** Structured data containing the query, the expected answer, and metadata.\n",
    "\n",
    "### Data Pipelines\n",
    "\n",
    "**1. Preprocessing Pipeline:**\n",
    "\n",
    "-   **Input:** Raw text documents.\n",
    "-   **Process:**\n",
    "    -   **Tokenization:** Documents are split into sentences.\n",
    "    -   **Chunking:** Sentences are grouped into chunks based on the `sentences_per_chunk` parameter.\n",
    "    -   **Embedding:** Chunks are converted into embeddings using a pre-trained model.\n",
    "-   **Output:** Embeddings stored in the `storage/embeddings/` directory.\n",
    "\n",
    "**2. KDTree Construction Pipeline:**\n",
    "\n",
    "-   **Input:** Precomputed embeddings.\n",
    "-   **Process:** Embeddings are indexed using KDTree for efficient retrieval.\n",
    "-   **Output:** A KDTree structure ready for fast nearest neighbor search.\n",
    "\n",
    "### Model\n",
    "\n",
    "**Embedding Model:**\n",
    "\n",
    "-   **Model Used:** Sentence Transformers (`all-MiniLM-L6-v2`).\n",
    "-   **Purpose:** Convert text chunks into high-dimensional embeddings capturing semantic meaning.\n",
    "\n",
    "**Retrieval Model:**\n",
    "\n",
    "-   **Model Used:** KDTree.\n",
    "-   **Purpose:** Efficient retrieval of contextually relevant chunks based on query embeddings.\n",
    "\n",
    "**Answer Generation Model:**\n",
    "\n",
    "-   **Model Used:** BERT (`bert-large-uncased-whole-word-masking-finetuned-squad`).\n",
    "-   **Purpose:** Generate answers by leveraging retrieved context chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Metrics**\n",
    "\n",
    "#### Offline Metrics\n",
    "\n",
    "**1. Match Result**\n",
    "\n",
    "- **Definition**: The `Match Result` metric indicates whether the answer generated by the system matches the ground truth answer. This is a binary metric where a match (True) is recorded if the generated answer is identical or sufficiently similar to the ground truth.\n",
    "\n",
    "- **Mathematical Notation**:\n",
    "  $$\n",
    "  \\text{Match Result} = \n",
    "  \\begin{cases} \n",
    "  1 & \\text{if } \\text{Generated Answer} = \\text{Ground Truth} \\\\\n",
    "  0 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Purpose**: The `Match Result` is a straightforward measure of accuracy. It is used to evaluate how often the system produces the correct answer. This metric is particularly important for understanding the basic correctness of the system's output.\n",
    "\n",
    "- **Intuition**: If the `Match Result` is consistently high, it suggests that the system is reliably providing the correct answers. It is the most direct way to assess the system's accuracy and is crucial for tasks where exact matches are critical.\n",
    "\n",
    "---\n",
    "\n",
    "**2. Score**\n",
    "\n",
    "- **Definition**: The `Score` metric quantifies the similarity between the generated answer and the ground truth. This score is usually derived from a similarity measure such as cosine similarity, BLEU, or ROUGE.\n",
    "\n",
    "- **Mathematical Notation** (for Cosine Similarity):\n",
    "  $$\n",
    "  \\text{Score} = \\cos(\\theta) = \\frac{\\text{Generated Answer} \\cdot \\text{Ground Truth}}{\\|\\text{Generated Answer}\\| \\|\\text{Ground Truth}\\|}\n",
    "  $$\n",
    "  Where \\( \\theta \\) is the angle between the embedding vectors of the generated answer and the ground truth.\n",
    "\n",
    "- **Purpose**: The `Score` provides a more nuanced evaluation than the `Match Result`. It measures how similar the generated answer is to the ground truth, even if it is not an exact match. This is useful for assessing the quality of answers in scenarios where exact matches are rare, but semantic similarity is important.\n",
    "\n",
    "- **Intuition**: A high `Score` indicates that even when the answer isn't exactly correct, it is close in meaning or content to the correct answer. This metric helps in cases where multiple valid answers might exist, allowing for flexibility in evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "**3. Precision**\n",
    "\n",
    "- **Definition**: Precision is the fraction of relevant instances among the retrieved instances. In the context of the RAG system, it measures how many of the retrieved context chunks were actually relevant to the query.\n",
    "\n",
    "- **Mathematical Notation**:\n",
    "$$\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "  $$\n",
    "\n",
    "- **Purpose**: Precision helps assess the relevance of the retrieved information. It is particularly important in systems where the cost of retrieving irrelevant information is high, such as in information retrieval systems.\n",
    "\n",
    "- **Intuition**: High precision indicates that the system is good at filtering out irrelevant information, which is crucial for generating accurate and concise answers.\n",
    "\n",
    "---\n",
    "\n",
    "**4. Recall**\n",
    "\n",
    "- **Definition**: Recall is the fraction of relevant instances that were retrieved out of all relevant instances. It measures how many of the relevant context chunks were retrieved by the system.\n",
    "\n",
    "- **Mathematical Notation**:\n",
    " $$\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "  $$\n",
    "\n",
    "- **Purpose**: Recall evaluates the system's ability to retrieve all relevant information. This metric is important for ensuring that the system doesn't miss critical information that could contribute to generating a correct answer.\n",
    "\n",
    "- **Intuition**: High recall indicates that the system is thorough in retrieving potentially useful information, which is important for generating comprehensive answers.\n",
    "\n",
    "---\n",
    "\n",
    "**5. F1-Score**\n",
    "\n",
    "- **Definition**: The F1-Score is the harmonic mean of precision and recall, providing a single metric that balances the two. It is especially useful when you need to balance the trade-off between precision and recall.\n",
    "\n",
    "- **Mathematical Notation**:\n",
    "  $$\n",
    "  F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "  $$\n",
    "\n",
    "- **Purpose**: The F1-Score is used when there is a need to balance precision and recall. It is particularly valuable in scenarios where both false positives and false negatives are costly, such as in question-answering systems where both the relevance and completeness of retrieved information matter.\n",
    "\n",
    "- **Intuition**: A high F1-Score indicates that the system is performing well in terms of both precision and recall, making it a good overall measure of retrieval quality.\n",
    "\n",
    "---\n",
    "\n",
    "#### Online Metrics\n",
    "\n",
    "**1. Response Time**\n",
    "\n",
    "- **Definition**: The time taken by the system to process a query and return an answer.\n",
    "\n",
    "- **Mathematical Notation**:\n",
    " $$\n",
    "  \\text{Response Time} = \\text{Time of Response} - \\text{Time of Query}\n",
    "  $$\n",
    "\n",
    "- **Purpose**: Response time is crucial for user experience, especially in interactive applications where users expect quick responses. It is an essential metric for ensuring that the system remains performant under various load conditions.\n",
    "\n",
    "- **Intuition**: Lower response times generally lead to better user satisfaction, as users are more likely to trust and use a system that responds quickly.\n",
    "\n",
    "---\n",
    "\n",
    "**2. User Satisfaction**\n",
    "\n",
    "- **Definition**: A qualitative metric that measures how satisfied users are with the system's performance. This is usually gathered through surveys, feedback forms, or direct interactions.\n",
    "\n",
    "- **Purpose**: User satisfaction provides insight into how well the system is meeting the needs and expectations of its users. While it is a qualitative metric, it is critical for understanding the real-world impact of the system.\n",
    "\n",
    "- **Intuition**: High user satisfaction indicates that the system is not only technically sound but also aligns with user expectations and requirements, making it a key success metric for deployed systems.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion on Metrics\n",
    "\n",
    "These metrics are carefully chosen to cover different aspects of system performance. Offline metrics such as `Match Result`, `Score`, `Precision`, `Recall`, and `F1-Score` are critical for evaluating the system during development and ensuring that it meets accuracy and relevance standards. Online metrics like `Response Time` and `User Satisfaction` help monitor the system's performance in real-world scenarios, ensuring that it remains responsive and user-friendly after deployment.\n",
    "\n",
    "Together, these metrics provide a comprehensive view of the system's effectiveness, allowing for continuous improvement and fine-tuning based on both technical performance and user feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Post-Deployment Policies (Detailed)\n",
    "\n",
    "Effective post-deployment strategies are relevant for maintaining the reliability, performance, and accuracy of the system. Below are some key aspects of monitoring, maintenance, and fault mitigation, ensuring that the system remains robust and responsive in a production environment.\n",
    "\n",
    "\n",
    "#### 1. Monitoring and Maintenance Plan\n",
    "\n",
    "**1.1 Real-time Monitoring**\n",
    "\n",
    "**Purpose**: Real-time monitoring is essential for tracking the health and performance of the system. By continuously collecting and analyzing data from various system components, one detect anomalies, performance degradations, or failures as they occur, allowing for quick intervention.\n",
    "\n",
    "**Components to Monitor**:\n",
    "\n",
    "-   **API Latency and Response Times**: Track how long it takes for the system to process queries and return answers. Sudden spikes in latency can indicate performance bottlenecks or underlying issues in the infrastructure.\n",
    "-   **Error Rates**: Monitor the frequency and types of errors (e.g., 4xx/5xx HTTP responses) that occur. A rising error rate could signal issues with the API endpoints, model inference, or data retrieval.\n",
    "-   **Resource Utilization**: Keep an eye on CPU, memory, disk I/O, and network usage across all services. High resource consumption might indicate inefficiencies or the need for scaling.\n",
    "-   **Request Volume**: Track the number of requests per minute/hour. Sudden changes in request volume can help identify unexpected load conditions or usage patterns.\n",
    "\n",
    "**1.2 Error Logging and Analysis**\n",
    "\n",
    "**Purpose**: Comprehensive logging is vital for diagnosing issues and understanding the sequence of events leading up to a problem. By keeping detailed logs, you can perform post-mortem analyses and continuously improve the system.\n",
    "\n",
    "**Logging Strategy**:\n",
    "\n",
    "-   **Request Logging**: Log all incoming API requests, including request data, timestamps, and IP addresses. This helps in tracing and replicating issues.\n",
    "-   **Error and Exception Logging**: Capture stack traces and error messages from failed operations or unhandled exceptions. Include contextual information to aid in debugging.\n",
    "-   **Model Inference Logs**: Log input queries, selected context chunks, and generated answers, along with corresponding confidence scores and inference times.\n",
    "\n",
    "**Storage and Retention**:\n",
    "\n",
    "-   **Local Storage**: Use a dedicated storage solution for logs, such as a logging server with high availability.\n",
    "-   **Cloud Storage**: For scalability, consider cloud-based log storage solutions like AWS CloudWatch or Google Cloud Logging.\n",
    "-   **Retention Policy**: Retain logs for a sufficient period (e.g., 30-90 days) to allow for historical analysis while managing storage costs.\n",
    "\n",
    "**1.3 Health Checks**\n",
    "\n",
    "**Purpose**: Health checks ensure that the system's components are functioning correctly. By regularly probing the system, you can detect failures early and trigger alerts or automatic recovery mechanisms.\n",
    "\n",
    "**Types of Health Checks**:\n",
    "\n",
    "-   **API Health Checks**: Periodically ping the API endpoints to verify their responsiveness and correctness.\n",
    "-   **Model Health Checks**: Run lightweight inference tasks at regular intervals to confirm that the models are operating as expected.\n",
    "-   **Database/Storage Health Checks**: Ensure that the KDTree, embeddings, and log storage systems are accessible and performing optimally.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "-   **Automated Scripts**: Deploy cron jobs or similar scheduling tools to run health checks at regular intervals.\n",
    "-   **Service Mesh**: Use a service mesh like Istio to manage and monitor microservices health, automatically routing traffic away from unhealthy instances.\n",
    "\n",
    "----------\n",
    "\n",
    "#### 2. Fault Mitigation Strategies\n",
    "\n",
    "**2.1 Graceful Degradation**\n",
    "\n",
    "**Purpose**: In the event of a failure, the system should continue to operate at a reduced capacity rather than failing completely. Graceful degradation ensures that users still receive a response, even if it is less detailed or accurate.\n",
    "\n",
    "**Strategies**:\n",
    "\n",
    "-   **Fallback Mechanisms**: Implement fallback methods such as brute-force search or cached responses if KDTree retrieval fails. This allows the system to return a reasonable answer even if the primary retrieval mechanism is unavailable.\n",
    "-   **Reduced Load Operations**: Temporarily reduce the complexity of operations (e.g., lowering the `top_k` value or using simpler models) during high-load scenarios to maintain service availability.\n",
    "-   **Partial Responses**: If only some components fail (e.g., partial retrieval of context), return the partial result with a notification to the user, rather than failing entirely.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "-   **Circuit Breaker Patterns**: Use circuit breaker patterns to detect and isolate failing components, allowing the system to bypass them and continue functioning.\n",
    "-   **Rate Limiting**: Implement rate limiting to protect the system from overwhelming traffic, ensuring that critical services remain available.\n",
    "\n",
    "**2.2 Load Balancing**\n",
    "\n",
    "**Purpose**: Load balancing distributes incoming requests across multiple instances of the system to ensure that no single instance becomes a bottleneck. This enhances both reliability and performance.\n",
    "\n",
    "**Strategies**:\n",
    "\n",
    "-   **Horizontal Scaling**: Deploy multiple instances of the API and retrieval services. Use a load balancer (e.g., NGINX, AWS ELB) to distribute traffic evenly.\n",
    "-   **Geographic Load Balancing**: For global deployments, distribute traffic based on user location to reduce latency and balance load across regions.\n",
    "-   **Auto-Scaling**: Automatically scale the number of instances based on current load. Tools like AWS Auto Scaling or Kubernetes Horizontal Pod Autoscaler can manage this dynamically.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "-   **Session Affinity**: Use session affinity (sticky sessions) if needed to ensure that users continue interacting with the same instance during a session.\n",
    "-   **Monitoring and Alerts**: Continuously monitor the performance of load balancers and trigger alerts if imbalances or failures are detected.\n",
    "\n",
    "**2.3 Redundancy**\n",
    "\n",
    "**Purpose**: Redundancy ensures that critical system components have backups, allowing the system to recover quickly from failures and minimizing downtime.\n",
    "\n",
    "**Strategies**:\n",
    "\n",
    "-   **Data Redundancy**: Store multiple copies of critical data (e.g., embeddings, logs) across different storage locations or cloud regions. This protects against data loss due to hardware failure or regional outages.\n",
    "-   **Service Redundancy**: Deploy multiple instances of key services (e.g., Flask API, KDTree) across different servers or availability zones. If one instance fails, traffic can be routed to the redundant instance.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "-   **Replication and Mirroring**: Use database replication or file system mirroring to maintain up-to-date copies of critical data in different locations.\n",
    "-   **Failover Mechanisms**: Implement automatic failover mechanisms to switch to a backup service or data store in case the primary one fails.\n",
    "\n",
    "**2.4 Automated Alerts**\n",
    "\n",
    "**Purpose**: Automated alerts notify the operations team of critical issues, enabling rapid response and minimizing downtime.\n",
    "\n",
    "**Strategies**:\n",
    "\n",
    "-   **Threshold-Based Alerts**: Set thresholds for key metrics (e.g., CPU usage, error rates, response times). Trigger alerts if these thresholds are breached.\n",
    "-   **Anomaly Detection**: Use machine learning-based anomaly detection to identify unusual patterns in system behavior that may indicate an emerging issue.\n",
    "-   **On-Call Rotation**: Establish an on-call rotation for team members to ensure that someone is always available to respond to alerts.\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "-   **Alerting Tools**: Use tools like PagerDuty, Opsgenie, or custom scripts to send alerts via email, SMS, or messaging apps (e.g., Slack).\n",
    "-   **Incident Management**: Integrate alerting systems with incident management tools to track and resolve issues systematically.\n",
    "\n",
    "----------\n",
    "\n",
    "### Conclusion on Post-Deployment Policies\n",
    "\n",
    "The post-deployment policies outlined above are designed to ensure the continuous, reliable operation of the RAG system in a production environment. By implementing robust monitoring, proactive maintenance, and comprehensive fault mitigation strategies, the system can maintain high availability, performance, and accuracy. These policies are essential for providing a dependable user experience, quickly addressing issues as they arise, and continuously improving the system based on real-world usage data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
